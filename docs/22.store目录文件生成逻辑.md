# Storeç›®å½•æ–‡ä»¶ç”Ÿæˆé€»è¾‘åˆ†æ

## ğŸ¯ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æCoorAgentç³»ç»Ÿä¸­storeç›®å½•çš„æ–‡ä»¶ç»„ç»‡ç»“æ„å’Œç”Ÿæˆé€»è¾‘ï¼Œé‡ç‚¹åˆ†ææ™ºèƒ½ä½“åˆ›å»ºååœ¨å­˜å‚¨ç³»ç»Ÿä¸­å¦‚ä½•ç”Ÿæˆã€ç»„ç»‡å’Œç®¡ç†ç›¸å…³æ–‡ä»¶ã€‚Storeç›®å½•æ˜¯CoorAgentæŒä¹…åŒ–å­˜å‚¨çš„æ ¸å¿ƒï¼ŒåŒ…å«æ™ºèƒ½ä½“é…ç½®ã€æç¤ºè¯ã€å·¥ä½œæµçŠ¶æ€ç­‰å…³é”®æ•°æ®ã€‚

## ğŸ“ Storeç›®å½•ç»“æ„æ¦‚è§ˆ

```
store/
â”œâ”€â”€ agents/              # æ™ºèƒ½ä½“é…ç½®æ–‡ä»¶å­˜å‚¨
â”‚   â”œâ”€â”€ researcher.json     # ç ”ç©¶å‘˜æ™ºèƒ½ä½“é…ç½®
â”‚   â”œâ”€â”€ coder.json          # ç¼–ç¨‹æ™ºèƒ½ä½“é…ç½®
â”‚   â”œâ”€â”€ reporter.json       # æŠ¥å‘Šå‘˜æ™ºèƒ½ä½“é…ç½®
â”‚   â”œâ”€â”€ itinerary_designer.json  # è¡Œç¨‹è®¾è®¡æ™ºèƒ½ä½“é…ç½®
â”‚   â””â”€â”€ ...                 # å…¶ä»–æ™ºèƒ½ä½“é…ç½®
â”œâ”€â”€ prompts/             # æ™ºèƒ½ä½“æç¤ºè¯æ–‡ä»¶å­˜å‚¨
â”‚   â”œâ”€â”€ researcher.md       # ç ”ç©¶å‘˜æç¤ºè¯
â”‚   â”œâ”€â”€ coder.md           # ç¼–ç¨‹æ™ºèƒ½ä½“æç¤ºè¯
â”‚   â”œâ”€â”€ reporter.md        # æŠ¥å‘Šå‘˜æç¤ºè¯
â”‚   â”œâ”€â”€ itinerary_designer.md  # è¡Œç¨‹è®¾è®¡æ™ºèƒ½ä½“æç¤ºè¯
â”‚   â””â”€â”€ ...                # å…¶ä»–æç¤ºè¯æ–‡ä»¶
â”œâ”€â”€ workflows/           # å·¥ä½œæµçŠ¶æ€æ–‡ä»¶å­˜å‚¨
â”‚   â””â”€â”€ {user_id}/       # æŒ‰ç”¨æˆ·IDåˆ†ç»„
â”‚       â”œâ”€â”€ {workflow_hash}.json  # å·¥ä½œæµçŠ¶æ€æ–‡ä»¶
â”‚       â””â”€â”€ ...
â””â”€â”€ tools/               # å·¥å…·é…ç½®æ–‡ä»¶å­˜å‚¨ï¼ˆé¢„ç•™ï¼‰
```

## ğŸ”§ è·¯å¾„é…ç½®ç³»ç»Ÿ

### 1. å…¨å±€è·¯å¾„é…ç½®

**æ–‡ä»¶**: `config/global_variables.py`

```python
from src.utils.path_utils import get_project_root

# å„ç›®å½•è·¯å¾„é…ç½®
workflow_dir = get_project_root() / "store" / "workflows"
tools_dir = get_project_root() / "store" / "tools"
agents_dir = get_project_root() / "store" / "agents"
prompts_dir = get_project_root() / "store" / "prompts"
workflows_dir = get_project_root() / "store" / "workflows"
```

### 2. é¡¹ç›®æ ¹ç›®å½•ç¡®å®š

**æ–‡ä»¶**: `src/utils/path_utils.py`

```python
@lru_cache(maxsize=None)
def get_project_root() -> Path:
    """
    å¤šç­–ç•¥ç¡®å®šé¡¹ç›®æ ¹ç›®å½•:
    1. ä»å½“å‰æ–‡ä»¶å‘ä¸Šæœç´¢ .git, pyproject.toml, .project-root
    2. ä»å·¥ä½œç›®å½•å‘ä¸Šæœç´¢
    3. ä½¿ç”¨å®‰è£…è·¯å¾„ä½œä¸ºåå¤‡æ–¹æ¡ˆ
    """
    current_path = Path(__file__).parent.absolute()
    max_depth = 10
    
    for _ in range(max_depth):
        if (current_path / '.git').exists() or \
           (current_path / 'pyproject.toml').exists() or \
           (current_path / '.project-root').exists():
            return current_path
        current_path = current_path.parent
    
    return Path(__file__).parent.parent.parent
```

### 3. AgentManagerè·¯å¾„åˆå§‹åŒ–

**æ–‡ä»¶**: `src/manager/agents.py` (ç¬¬259-268è¡Œ)

```python
from src.utils.path_utils import get_project_root

# åˆå§‹åŒ–å„ç›®å½•è·¯å¾„
tools_dir = get_project_root() / "store" / "tools"
agents_dir = get_project_root() / "store" / "agents"
prompts_dir = get_project_root() / "store" / "prompts"

# åˆ›å»ºAgentManagerå®ä¾‹
agent_manager = AgentManager(tools_dir, agents_dir, prompts_dir)
asyncio.run(agent_manager.initialize())
```

## ğŸ¤– æ™ºèƒ½ä½“æ–‡ä»¶ç”Ÿæˆé€»è¾‘

### 1. æ™ºèƒ½ä½“åˆ›å»ºæµç¨‹

```mermaid
graph TD
    A[LLMç”ŸæˆAgentBuilder] --> B[Agentå¯¹è±¡åˆ›å»º]
    B --> C[_save_agentæ–¹æ³•è°ƒç”¨]
    C --> D[ç”ŸæˆJSONé…ç½®æ–‡ä»¶]
    C --> E[ç”ŸæˆMDæç¤ºè¯æ–‡ä»¶]
    D --> F[ä¿å­˜åˆ°agentsç›®å½•]
    E --> G[ä¿å­˜åˆ°promptsç›®å½•]
    F --> H[æ·»åŠ åˆ°available_agents]
    G --> H
```

### 2. _save_agentæ–¹æ³•å®ç°

**æ–‡ä»¶**: `src/manager/agents.py` (ç¬¬109-126è¡Œ)

```python
async def _save_agent(self, agent: Agent, flush=False):
    """ä¿å­˜æ™ºèƒ½ä½“é…ç½®å’Œæç¤ºè¯æ–‡ä»¶"""
    # æ„å»ºæ–‡ä»¶è·¯å¾„
    agent_path = self.agents_dir / f"{agent.agent_name}.json"
    agent_prompt_path = self.prompt_dir / f"{agent.agent_name}.md"
    agents = []

    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–‡ä»¶ï¼ˆflush=Trueä¸”æ–‡ä»¶ä¸å­˜åœ¨ï¼‰
    if flush and not agent_path.exists():
        # åºåˆ—åŒ–Agentå¯¹è±¡ä¸ºJSONæ ¼å¼
        agents.append((agent_path, agent.model_dump_json(indent=4)))

    if flush and not agent_prompt_path.exists():
        # ä¿å­˜æç¤ºè¯å†…å®¹
        agents.append((agent_prompt_path, agent.prompt))

    if not agents:
        logger.debug(f"skip saving agent")
        return

    # å¼‚æ­¥å¹¶å‘å†™å…¥æ–‡ä»¶
    agent_tasks = [self._write_file(path, content) for path, content in agents]
    await asyncio.gather(*agent_tasks)

    logger.info(f"agent {agent.agent_name} saved.")
```

### 3. æ–‡ä»¶å†™å…¥å®ç°

**æ–‡ä»¶**: `src/manager/agents.py` (ç¬¬104-107è¡Œ)

```python
async def _write_file(self, path: Path, content: str):
    """ä½¿ç”¨UTF-8ç¼–ç å¼‚æ­¥å†™å…¥æ–‡ä»¶"""
    async with aiofiles.open(path, "w", encoding="utf-8") as f:
        await f.write(content)
```

### 4. Agentæ¨¡å‹ç»“æ„

**æ–‡ä»¶**: `src/interface/agent.py` (ç¬¬86-104è¡Œ)

```python
class Agent(BaseModel):
    """æ™ºèƒ½ä½“å®šä¹‰æ¨¡å‹"""
    user_id: str              # ç”¨æˆ·ID
    agent_name: str           # æ™ºèƒ½ä½“åç§°
    nick_name: str            # æ˜µç§°
    description: str          # æè¿°
    llm_type: LLMType        # LLMç±»å‹
    selected_tools: List[Tool]  # é€‰æ‹©çš„å·¥å…·åˆ—è¡¨
    prompt: str              # æç¤ºè¯
    model_config = ConfigDict(extra="allow")
```

## ğŸ“„ ç”Ÿæˆæ–‡ä»¶æ ¼å¼åˆ†æ

### 1. JSONé…ç½®æ–‡ä»¶æ ¼å¼

**ç¤ºä¾‹**: `store/agents/itinerary_designer.json`

```json
{
    "user_id": "test",
    "agent_name": "itinerary_designer",
    "nick_name": "itinerary_designer",
    "description": "ä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿæ ¹æ®ç›®çš„åœ°å’Œç”¨æˆ·åå¥½æ¨èæ™¯ç‚¹ï¼Œæä¾›æ¨èç†ç”±åŠç›¸å…³ç…§ç‰‡ URLï¼Œå¹¶ç”Ÿæˆè¯¦ç»†æ—¥ç¨‹å®‰æ’ã€‚",
    "llm_type": "reasoning",
    "selected_tools": [
        {
            "name": "tavily_tool",
            "description": "A search engine optimized for comprehensive, accurate, and trusted results."
        },
        {
            "name": "crawl_tool", 
            "description": "Use this to crawl a url and get a readable content in markdown format."
        }
    ],
    "prompt": "# Role: è¡Œç¨‹è®¾è®¡æ™ºèƒ½ä½“ (Itinerary Designer)\\nä½ æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–è¡Œç¨‹è®¾è®¡çš„æ™ºèƒ½åŠ©æ‰‹..."
}
```

**å­—æ®µè¯´æ˜**:
- `user_id`: æ™ºèƒ½ä½“å½’å±ç”¨æˆ·ï¼Œ"share"è¡¨ç¤ºå…±äº«æ™ºèƒ½ä½“
- `agent_name`: å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºæ–‡ä»¶å‘½åå’Œå¼•ç”¨
- `nick_name`: æ˜¾ç¤ºåç§°
- `description`: åŠŸèƒ½æè¿°ï¼Œç”¨äºå·¥ä½œæµä¸­çš„æ™ºèƒ½ä½“é€‰æ‹©
- `llm_type`: æŒ‡å®šä½¿ç”¨çš„LLMç±»å‹ï¼ˆbasic, reasoningç­‰ï¼‰
- `selected_tools`: æ™ºèƒ½ä½“å¯ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨
- `prompt`: å®Œæ•´çš„æç¤ºè¯å†…å®¹ï¼ŒåŒ…å«è½¬ä¹‰å­—ç¬¦

### 2. Markdownæç¤ºè¯æ–‡ä»¶æ ¼å¼

**ç¤ºä¾‹**: `store/prompts/itinerary_designer.md`

```markdown
# Role: è¡Œç¨‹è®¾è®¡æ™ºèƒ½ä½“ (Itinerary Designer)
ä½ æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–è¡Œç¨‹è®¾è®¡çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®ç›®çš„åœ°å’Œç”¨æˆ·çš„å…´è¶£åå¥½æ¨èæ™¯ç‚¹ï¼Œé™„å¸¦æ¨èç†ç”±ã€ç…§ç‰‡ URLï¼Œå¹¶åˆ¶å®šè¯¦ç»†æ—¥ç¨‹ã€‚
ç›®æ ‡æ˜¯ä¸ºç”¨æˆ·æä¾›ä¸€ä»½æ¸…æ™°æ˜“è¯»ä¸”å®ç”¨çš„æ—…è¡Œè®¡åˆ’ã€‚

## æ­¥éª¤:
1. **æ¥æ”¶ç”¨æˆ·è¾“å…¥**ï¼šè·å–ç”¨æˆ·æä¾›çš„ç›®çš„åœ°å’Œå…´è¶£åå¥½ï¼ˆå¦‚è‡ªç„¶é£å…‰ã€å†å²æ–‡åŒ–ã€ç¾é£Ÿç­‰ï¼‰ã€‚
2. **ä½¿ç”¨ tavily_tool æœç´¢æ™¯ç‚¹ä¿¡æ¯**ï¼š
   - æ ¹æ®ç›®çš„åœ°å’Œåå¥½æœç´¢å‡ºçƒ­é—¨æ™¯ç‚¹åŠç›¸å…³ä¿¡æ¯ã€‚
   - æ”¶é›†æ¯ä¸ªæ™¯ç‚¹çš„æ¨èç†ç”±ï¼Œä¾‹å¦‚ä¸ºä»€ä¹ˆé€‚åˆæŸç§å…´è¶£ç±»å‹æˆ–ç‹¬ç‰¹ä¹‹å¤„ã€‚
3. **ä½¿ç”¨ crawl_tool è·å–ç…§ç‰‡ URL**ï¼š
   - å¯¹äºæ¯ä¸ªæ¨èæ™¯ç‚¹ï¼Œæ‰¾åˆ°é«˜è´¨é‡çš„ç›¸å…³å›¾ç‰‡é“¾æ¥ã€‚
...
```

**ç‰¹ç‚¹**:
- çº¯æ–‡æœ¬Markdownæ ¼å¼ï¼Œä¾¿äºç¼–è¾‘å’Œç‰ˆæœ¬æ§åˆ¶
- åŒ…å«å®Œæ•´çš„è§’è‰²å®šä¹‰ã€ä»»åŠ¡æè¿°ã€æ‰§è¡Œæ­¥éª¤
- æ”¯æŒå¤šè¯­è¨€å†…å®¹
- å¯åŒ…å«æ ¼å¼åŒ–æŒ‡ä»¤å’Œæ³¨æ„äº‹é¡¹

## ğŸ”„ å·¥ä½œæµæ–‡ä»¶ç”Ÿæˆé€»è¾‘

### 1. WorkflowCacheç±»è®¾è®¡

**æ–‡ä»¶**: `src/workflow/cache.py` (ç¬¬16-36è¡Œ)

```python
class WorkflowCache:
    """å·¥ä½œæµç¼“å­˜ç®¡ç†å™¨ - å•ä¾‹æ¨¡å¼"""
    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(WorkflowCache, cls).__new__(cls)
        return cls._instance

    def __init__(self, workflow_dir: Path):
        if not hasattr(self, 'initialized'):
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            if not workflow_dir.exists():
                logger.info(f"path {workflow_dir} does not exist when workflow cache initializing, gona to create...")
                workflow_dir.mkdir(parents=True, exist_ok=True)
            
            self.workflow_dir = workflow_dir
            self.queue = {}              # æ‰§è¡Œé˜Ÿåˆ—
            self.cache = {}              # å†…å­˜ç¼“å­˜
            self.latest_polish_id = {}   # æœ€æ–°ä¼˜åŒ–ID
            self.initialized = True
            self._lock_pool = {}         # çº¿ç¨‹é”æ± 
```

### 2. å·¥ä½œæµIDç”Ÿæˆè§„åˆ™

**æ–‡ä»¶**: `src/workflow/process.py` (ç¬¬76-82è¡Œ)

```python
# ç”Ÿæˆå·¥ä½œæµID
if not workflow_id:
    if workmode == "launch":
        # åŸºäºç”¨æˆ·è¾“å…¥ç”ŸæˆMD5å“ˆå¸Œ
        msg = f"{user_id}_{task_type}_{user_input_messages}_{deep_thinking_mode}_{search_before_planning}_{coor_agents}"
        polish_id = hashlib.md5(msg.encode("utf-8")).hexdigest()
    else:
        polish_id = cache.get_latest_polish_id(user_id)
    
    workflow_id = f"{user_id}:{polish_id}"
```

**å‘½åè§„åˆ™**:
- æ ¼å¼: `{user_id}:{polish_id}`
- polish_id: 32ä½MD5å“ˆå¸Œå€¼ï¼ŒåŸºäºç”¨æˆ·è¾“å…¥å†…å®¹ç”Ÿæˆ
- ç¡®ä¿ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒçš„å·¥ä½œæµIDï¼Œæ”¯æŒå¹‚ç­‰æ€§

### 3. å·¥ä½œæµæ–‡ä»¶ä¿å­˜æœºåˆ¶

**æ–‡ä»¶**: `src/workflow/cache.py` (ç¬¬307-339è¡Œ)

```python
def save_workflow(self, workflow):
    """ä¿å­˜å·¥ä½œæµåˆ°æ–‡ä»¶ç³»ç»Ÿ"""
    try:
        # è§£æå·¥ä½œæµID
        user_id, polish_id = workflow["workflow_id"].split(":")
        workflow_path = self.workflow_dir / user_id / f"{polish_id}.json"

        # çº¿ç¨‹å®‰å…¨å†™å…¥
        if user_id not in self._lock_pool:
            self._lock_pool[user_id] = threading.Lock()
        
        with self._lock_pool[user_id]:
            with open(workflow_path, "w", encoding='utf-8') as f:
                f.write(json.dumps(workflow, indent=2, ensure_ascii=False))
                
    except Exception as e:
        logger.error(f"Error dumping workflow: {e}")
    
    logger.info(f"workflow {workflow['workflow_id']} saved.")

def dump(self, workflow_id: str, mode: str):
    """æ ¹æ®æ¨¡å¼ä¿å­˜å·¥ä½œæµ"""
    try:
        workflow = self.cache[workflow_id]
        user_id, polish_id = workflow["workflow_id"].split(":")
        
        if user_id not in self._lock_pool:
            self._lock_pool[user_id] = threading.Lock()
            
        with self._lock_pool[user_id]:
            if mode == "launch":
                # é¦–æ¬¡å¯åŠ¨æ¨¡å¼ï¼šä¿å­˜å®Œæ•´å·¥ä½œæµ
                workflow_path = self.workflow_dir / user_id / f"{polish_id}.json"
                with open(workflow_path, "w", encoding='utf-8') as f:
                    f.write(json.dumps(workflow, indent=2, ensure_ascii=False))
                self.latest_polish_id[user_id] = polish_id
            elif mode == "production":
                # ç”Ÿäº§æ¨¡å¼ï¼šåˆå§‹åŒ–æ‰§è¡Œé˜Ÿåˆ—
                self.queue[workflow_id] = []
                
    except Exception as e:
        logger.error(f"Error dumping workflow: {e}")
```

### 4. ç”¨æˆ·ç›®å½•è‡ªåŠ¨åˆ›å»º

**æ–‡ä»¶**: `src/workflow/cache.py` (ç¬¬39-58è¡Œ)

```python
def _load_workflow(self, user_id: str):
    """åŠ è½½ç”¨æˆ·å·¥ä½œæµï¼Œè‡ªåŠ¨åˆ›å»ºç›®å½•"""
    try:
        if user_id not in self._lock_pool:
            self._lock_pool[user_id] = threading.Lock()
            
        with self._lock_pool[user_id]:
            user_workflow_dir = self.workflow_dir / user_id
            
            if not user_workflow_dir.exists():
                # ç”¨æˆ·ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºç›®å½•
                logger.info(f"path {user_workflow_dir} does not exist when user {user_id} workflow cache initializing, gona to create...")
                user_workflow_dir.mkdir(parents=True, exist_ok=True)
                return

            # åŠ è½½ç°æœ‰å·¥ä½œæµæ–‡ä»¶
            user_workflow_files = user_workflow_dir.glob("*.json")
            for workflow_file in user_workflow_files:
                with open(workflow_file, "r", encoding='utf-8') as f:
                    workflow = json.load(f)
                    self.cache[workflow["workflow_id"]] = workflow
                    
    except Exception as e:
        logger.error(f"Error loading workflow: {e}")
        raise e
```

## ğŸ“Š å·¥ä½œæµæ–‡ä»¶æ ¼å¼åˆ†æ

### 1. å·¥ä½œæµJSONç»“æ„

**ç¤ºä¾‹**: `store/workflows/test/71ac93050e4d0f733cedeb9a6e77333d.json`

```json
{
  "workflow_id": "test:71ac93050e4d0f733cedeb9a6e77333d",
  "mode": "launch",
  "version": 1,
  "lap": 1,
  "user_input_messages": [
    {
      "role": "user",
      "content": "åˆ›å»ºè¡Œç¨‹è®¾è®¡æ™ºèƒ½ä½“ï¼šæ ¹æ®ç›®çš„åœ°å’Œç”¨æˆ·åå¥½ï¼Œæ¨èæ™¯ç‚¹ã€ç»™å‡ºç†ç”±åŠç…§ç‰‡ URLï¼Œå¹¶è®¾è®¡è¯¦ç»†æ—¥ç¨‹ã€‚",
      "timestamp": "2025-07-27T16:44:25.663660"
    }
  ],
  "deep_thinking_mode": true,
  "search_before_planning": false,
  "coor_agents": [],
  "planning_steps": [
    {
      "agent_name": "agent_factory",
      "title": "åˆ›å»ºè¡Œç¨‹è®¾è®¡æ™ºèƒ½ä½“",
      "description": "ä½¿ç”¨ agent_factory åˆ›å»ºä¸€ä¸ªæ–°çš„æ™ºèƒ½ä½“...",
      "note": "ç¡®ä¿æ–°æ™ºèƒ½ä½“å…·å¤‡åˆ†æç”¨æˆ·åå¥½ã€æ•´åˆæ¨èå†…å®¹ã€ç”Ÿæˆæ—¥ç¨‹å®‰æ’çš„èƒ½åŠ›ã€‚"
    }
  ],
  "global_variables": {
    "has_lauched": "",
    "user_input": "",
    "history_messages": []
  },
  "memory": {
    "cache": {},
    "vector_store": {},
    "database": {},
    "file_store": {}
  },
  "graph": [
    {
      "component_type": "agent",
      "label": "agent_factory",
      "name": "agent_factory",
      "config": {
        "node_name": "agent_factory",
        "node_type": "execution_agent",
        "next_to": [],
        "condition": "supervised"
      }
    }
  ]
}
```

**å­—æ®µè¯¦è§£**:
- `workflow_id`: å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œæ ¼å¼ä¸º"ç”¨æˆ·ID:å“ˆå¸Œå€¼"
- `mode`: æ‰§è¡Œæ¨¡å¼ï¼ˆlaunch, production, polishï¼‰
- `version`: ç‰ˆæœ¬å·
- `lap`: æ‰§è¡Œè½®æ¬¡
- `user_input_messages`: åŸå§‹ç”¨æˆ·è¾“å…¥æ¶ˆæ¯
- `planning_steps`: è§„åˆ’çš„æ‰§è¡Œæ­¥éª¤
- `global_variables`: å…¨å±€å˜é‡çŠ¶æ€
- `memory`: å†…å­˜å­˜å‚¨åŒºåŸŸ
- `graph`: å·¥ä½œæµå›¾ç»“æ„ï¼ŒåŒ…å«èŠ‚ç‚¹å’Œè¿æ¥å…³ç³»

## ğŸ”’ çº¿ç¨‹å®‰å…¨å’Œå¹¶å‘æ§åˆ¶

### 1. çº¿ç¨‹é”æœºåˆ¶

```python
class WorkflowCache:
    def __init__(self, workflow_dir: Path):
        self._lock_pool = {}  # æŒ‰ç”¨æˆ·IDç»´æŠ¤é”æ± 
    
    def _get_user_lock(self, user_id: str):
        """è·å–ç”¨æˆ·ä¸“ç”¨é”"""
        if user_id not in self._lock_pool:
            self._lock_pool[user_id] = threading.Lock()
        return self._lock_pool[user_id]
```

### 2. æ–‡ä»¶å†™å…¥ä¿æŠ¤

```python
def save_workflow(self, workflow):
    user_id, polish_id = workflow["workflow_id"].split(":")
    
    # ä½¿ç”¨ç”¨æˆ·ä¸“ç”¨é”ä¿æŠ¤å†™å…¥æ“ä½œ
    with self._lock_pool[user_id]:
        with open(workflow_path, "w", encoding='utf-8') as f:
            f.write(json.dumps(workflow, indent=2, ensure_ascii=False))
```

**å®‰å…¨ç‰¹æ€§**:
- æŒ‰ç”¨æˆ·åˆ†ç¦»é”ï¼Œé¿å…ä¸åŒç”¨æˆ·é—´çš„é”ç«äº‰
- ä¿æŠ¤æ–‡ä»¶è¯»å†™æ“ä½œçš„åŸå­æ€§
- é˜²æ­¢å¹¶å‘è®¿é—®å¯¼è‡´çš„æ•°æ®æŸå

## ğŸ—‚ï¸ ç›®å½•ç»„ç»‡ç­–ç•¥

### 1. æŒ‰ç”¨æˆ·åˆ†ç»„

```
store/workflows/
â”œâ”€â”€ test/                    # æµ‹è¯•ç”¨æˆ·
â”‚   â”œâ”€â”€ 71ac93050e4d0f733cedeb9a6e77333d.json
â”‚   â”œâ”€â”€ f78ffbed02b2fa35e04e102ef9652c99.json
â”‚   â””â”€â”€ 8f693afeafa3a587c01c3812912a100e.json
â”œâ”€â”€ user123/                 # æ™®é€šç”¨æˆ·
â”‚   â”œâ”€â”€ abc123...def.json
â”‚   â””â”€â”€ ...
â””â”€â”€ admin/                   # ç®¡ç†å‘˜ç”¨æˆ·
    â”œâ”€â”€ 456def...789.json
    â””â”€â”€ ...
```

### 2. æ™ºèƒ½ä½“ç»Ÿä¸€å­˜å‚¨

```
store/agents/
â”œâ”€â”€ researcher.json          # å…±äº«æ™ºèƒ½ä½“
â”œâ”€â”€ coder.json              # å…±äº«æ™ºèƒ½ä½“
â”œâ”€â”€ itinerary_designer.json # ç”¨æˆ·è‡ªå®šä¹‰æ™ºèƒ½ä½“
â””â”€â”€ ...

store/prompts/
â”œâ”€â”€ researcher.md           # å¯¹åº”æç¤ºè¯
â”œâ”€â”€ coder.md               # å¯¹åº”æç¤ºè¯
â”œâ”€â”€ itinerary_designer.md  # å¯¹åº”æç¤ºè¯
â””â”€â”€ ...
```

**ç»„ç»‡åŸåˆ™**:
- å·¥ä½œæµæŒ‰ç”¨æˆ·éš”ç¦»ï¼Œé¿å…æ•°æ®æ··æ·†
- æ™ºèƒ½ä½“ç»Ÿä¸€å­˜å‚¨ï¼Œæ”¯æŒå…±äº«å’Œå¤ç”¨
- æ–‡ä»¶å‘½åè§„èŒƒåŒ–ï¼Œä¾¿äºæŸ¥æ‰¾å’Œç®¡ç†

## ğŸ”§ æ–‡ä»¶ç”Ÿå‘½å‘¨æœŸç®¡ç†

### 1. åˆ›å»ºæ—¶æœº

**æ™ºèƒ½ä½“æ–‡ä»¶**:
- Agent FactoryèŠ‚ç‚¹æˆåŠŸåˆ›å»ºæ™ºèƒ½ä½“æ—¶
- è°ƒç”¨`_create_agent_by_prebuilt`æ–¹æ³•æ—¶
- ä»…åœ¨flush=Trueä¸”æ–‡ä»¶ä¸å­˜åœ¨æ—¶åˆ›å»º

**å·¥ä½œæµæ–‡ä»¶**:
- å·¥ä½œæµåˆå§‹åŒ–æ—¶åˆ›å»ºåŸºç¡€ç»“æ„
- æ‰§è¡Œè¿‡ç¨‹ä¸­å®æ—¶æ›´æ–°çŠ¶æ€
- å®Œæˆæ—¶ä¿å­˜æœ€ç»ˆçŠ¶æ€

### 2. æ›´æ–°æœºåˆ¶

```python
async def _save_agent(self, agent: Agent, flush=False):
    # åªæœ‰flush=Trueä¸”æ–‡ä»¶ä¸å­˜åœ¨æ—¶æ‰åˆ›å»ºæ–°æ–‡ä»¶
    if flush and not agent_path.exists():
        agents.append((agent_path, agent.model_dump_json(indent=4)))
```

**æ›´æ–°ç­–ç•¥**:
- æ™ºèƒ½ä½“æ–‡ä»¶ï¼šåˆ›å»ºåä¸å†ä¿®æ”¹ï¼ˆå¹‚ç­‰æ€§ï¼‰
- å·¥ä½œæµæ–‡ä»¶ï¼šæ‰§è¡Œè¿‡ç¨‹ä¸­æŒç»­æ›´æ–°
- ä½¿ç”¨flushå‚æ•°æ§åˆ¶å†™å…¥æ—¶æœº

### 3. æ¸…ç†æœºåˆ¶

```python
async def _remove_agent(self, agent_name: str):
    """ç§»é™¤æ™ºèƒ½ä½“æ–‡ä»¶"""
    agent_path = self.agents_dir / f"{agent_name}.json"
    agent_prompt_path = self.prompt_dir / f"{agent_name}.md"

    if agent_path.exists():
        await aiofiles.os.remove(agent_path)
        logger.info(f"Removed agent definition file: {agent_path}")
    
    if agent_prompt_path.exists():
        await aiofiles.os.remove(agent_prompt_path)
        logger.info(f"Removed agent prompt file: {agent_prompt_path}")
    
    if agent_name in self.available_agents:
        del self.available_agents[agent_name]
        logger.info(f"Removed agent '{agent_name}' from available agents.")
```

## ğŸ” é”™è¯¯å¤„ç†å’Œæ¢å¤

### 1. ç¼–ç é—®é¢˜å¤„ç†

```python
# ä½¿ç”¨å¤šç¼–ç å®‰å…¨è¯»å–
from src.utils.encoding_utils import read_file_with_encoding

try:
    json_str = read_file_with_encoding(agent_path)
    _agent = Agent.model_validate_json(json_str)
except (UnicodeDecodeError, FileNotFoundError) as e:
    logger.error(f"Error reading agent file {agent_path}: {e}")
    raise
```

### 2. æ–‡ä»¶ç³»ç»Ÿé”™è¯¯å¤„ç†

```python
try:
    with open(workflow_path, "w", encoding='utf-8') as f:
        f.write(json.dumps(workflow, indent=2, ensure_ascii=False))
except Exception as e:
    logger.error(f"Error dumping workflow: {e}")
    # ä¸ä¸­æ–­ç¨‹åºæ‰§è¡Œï¼Œè®°å½•é”™è¯¯
```

### 3. ç›®å½•è‡ªåŠ¨æ¢å¤

```python
if not workflow_dir.exists():
    logger.info(f"path {workflow_dir} does not exist when workflow cache initializing, gona to create...")
    workflow_dir.mkdir(parents=True, exist_ok=True)
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. å¼‚æ­¥æ–‡ä»¶æ“ä½œ

```python
# ä½¿ç”¨aiofilesè¿›è¡Œå¼‚æ­¥I/O
async def _write_file(self, path: Path, content: str):
    async with aiofiles.open(path, "w", encoding="utf-8") as f:
        await f.write(content)

# å¹¶å‘å†™å…¥å¤šä¸ªæ–‡ä»¶
agent_tasks = [self._write_file(path, content) for path, content in agents]
await asyncio.gather(*agent_tasks)
```

### 2. å†…å­˜ç¼“å­˜æœºåˆ¶

```python
class WorkflowCache:
    def __init__(self, workflow_dir: Path):
        self.cache = {}  # å†…å­˜ç¼“å­˜å‡å°‘æ–‡ä»¶è¯»å–
        self.queue = {}  # æ‰§è¡Œé˜Ÿåˆ—ç¼“å­˜
```

### 3. å•ä¾‹æ¨¡å¼

```python
class WorkflowCache:
    _instance = None
    
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(WorkflowCache, cls).__new__(cls)
        return cls._instance
```

**ä¼˜åŒ–æ•ˆæœ**:
- å‡å°‘æ–‡ä»¶I/Oæ“ä½œ
- é¿å…é‡å¤å®ä¾‹åˆ›å»º
- æé«˜å¹¶å‘å¤„ç†èƒ½åŠ›

## ğŸ¯ æ€»ç»“

### å…³é”®è®¾è®¡æ¨¡å¼

1. **åˆ†å±‚å­˜å‚¨**: æŒ‰æ–‡ä»¶ç±»å‹å’Œç”¨æˆ·åˆ†ç¦»å­˜å‚¨
2. **å‘½åè§„èŒƒ**: åŸºäºå†…å®¹å“ˆå¸Œçš„ç¡®å®šæ€§å‘½å
3. **çº¿ç¨‹å®‰å…¨**: ç”¨æˆ·çº§åˆ«çš„é”æœºåˆ¶
4. **å¼‚æ­¥I/O**: æé«˜æ–‡ä»¶æ“ä½œæ€§èƒ½
5. **é”™è¯¯æ¢å¤**: è‡ªåŠ¨ç›®å½•åˆ›å»ºå’Œç¼–ç å¤„ç†
6. **ç¼“å­˜ä¼˜åŒ–**: å†…å­˜ç¼“å­˜å‡å°‘ç£ç›˜è®¿é—®

### å­˜å‚¨ä¼˜åŠ¿

1. **å¯è¿½æº¯æ€§**: å®Œæ•´ä¿å­˜å·¥ä½œæµæ‰§è¡Œå†å²
2. **å¯å¤ç°æ€§**: åŸºäºå“ˆå¸Œçš„ä¸€è‡´æ€§å‘½å
3. **å¯æ‰©å±•æ€§**: æ”¯æŒå¤šç”¨æˆ·å¹¶å‘æ“ä½œ
4. **å¯ç»´æŠ¤æ€§**: æ¸…æ™°çš„ç›®å½•ç»“æ„å’Œæ–‡ä»¶æ ¼å¼
5. **å¯æ¢å¤æ€§**: robustçš„é”™è¯¯å¤„ç†æœºåˆ¶

### æŠ€æœ¯ç‰¹ç‚¹

- **æŒä¹…åŒ–**: æ‰€æœ‰é‡è¦çŠ¶æ€éƒ½ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
- **ä¸€è‡´æ€§**: ä½¿ç”¨JSONæ ‡å‡†æ ¼å¼ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
- **å®‰å…¨æ€§**: UTF-8ç¼–ç å’Œçº¿ç¨‹å®‰å…¨ä¿éšœ
- **æ•ˆç‡**: å¼‚æ­¥I/Oå’Œå†…å­˜ç¼“å­˜ä¼˜åŒ–æ€§èƒ½
- **å¯è¯»æ€§**: äººç±»å¯è¯»çš„JSONå’ŒMarkdownæ ¼å¼

CoorAgentçš„å­˜å‚¨ç³»ç»Ÿé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ç›®å½•ç»“æ„å’Œæ–‡ä»¶ç”Ÿæˆé€»è¾‘ï¼Œå®ç°äº†æ™ºèƒ½ä½“é…ç½®ã€å·¥ä½œæµçŠ¶æ€çš„é«˜æ•ˆæŒä¹…åŒ–ç®¡ç†ï¼Œä¸ºç³»ç»Ÿçš„ç¨³å®šè¿è¡Œå’Œå¯æ‰©å±•æ€§æä¾›äº†åšå®åŸºç¡€ã€‚ 