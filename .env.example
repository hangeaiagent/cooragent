# LLM Environment variables

# Reasoning LLM (for complex reasoning tasks)
# If you're using your local Ollama, replace the model name after the slash and base url then you're good to go.
# For wider model support, read https://docs.litellm.ai/docs/providers.
REASONING_API_KEY=sk-1b811711996b40a0a7e232d153c26fab
REASONING_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
REASONING_MODEL=qwen-max-latest

# Non-reasoning LLM (for straightforward tasks)
BASIC_API_KEY=sk-1b811711996b40a0a7e232d153c26fab
BASIC_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
BASIC_MODEL=qwen-max-latest

 CODE_API_KEY=sk-14a4c2e8457d4bb4af82a815730fbf96
CODE_BASE_URL=https://api.deepseek.com/v1
CODE_MODEL=deepseek-chat

# VIDEO_MODEL=
# Vision-language LLM (for tasks requiring visual understanding)
VL_API_KEY=sk-1b811711996b40a0a7e232d153c26fab
VL_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
VL_MODEL=qwen2.5-vl-72b-instruct

# Application Settings
DEBUG=True
APP_ENV=development

# browser is default to False, for it's time consuming
USE_BROWSER=False
# Add other environment variables as needed
TAVILY_API_KEY=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
JINA_API_KEY=jina_5ada7cbe28bd4afc877a3e5e0cd1cd65rm5kuW5ySNmlQCdHUpVI3Lo510Lb # Optional, default is None

# turn off for collecting anonymous usage information
# ANONYMIZED_TELEMETRY=

# SLACK_USER_TOKEN=

#SILICONFLOW_API_KEY=

# Whether to use the agent of MCP tool, default to False
 MCP_AGENT=True

# The maximum execution steps of an agent,the default is 25,Non essential adjustments are not recommended
 MAX_STEPS = 125